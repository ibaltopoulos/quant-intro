{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "There are two main machine learning algorithms:\n",
    "* Supervised learning: Teach the computer how to do something\n",
    "* Unsupervised learning: We let the computer learn by itself\n",
    "\n",
    "Reinforcement learning/recommender systems.\n",
    "\n",
    "## Supervised Learning\n",
    "The most common type of machine learning algorithm.\n",
    "The term \"supervised\" means that we gave the algorithm the \"right answers\". For every example in the algorithm we gave it the \"right answer\".\n",
    "\n",
    "\n",
    "### Regression\n",
    "Predict continuous valued output (e.g. price of houses).\n",
    "\n",
    "We use a training set. The number of training examples is denoted by $m$. The input variables are denoted using $x$ and the output variables are denoted with $y$.\n",
    "\n",
    "$(x,y)$ denotes one training example\n",
    "\n",
    "$(x^{(i)}, y^{(i)})$ denotes the i-th training example\n",
    "\n",
    "The output of the learning algorithm is the learned function which is usually denoted by $h$ which stands for hypothesis. The function $h$, is a mapping $ x \\to y$.\n",
    "\n",
    "The representation of $h$ can vary. For linear regression we use $h_{\\theta}(x) = \\theta_0 + \\theta_1 x$. The $\\theta_i$ are the parameters of our model, and what we need to calculate.\n",
    "\n",
    "#### Cost function\n",
    "We choose the $\\theta_0, \\theta_1$ so that the value $h_\\theta (x)$ is close to $y$ for our training examples. To do that we define the cost function as:\n",
    "\n",
    "$$ J(\\theta_0, \\theta_1) = \\frac{1}{2m} \\sum_{i = 1}^m {(h_{\\theta}(x^{(i)}) - y^{(i)})^2 } $$\n",
    "\n",
    "This cost function is called the squared error function. And then we solve the following minimization problem:\n",
    "\n",
    "$$ \\underset{{\\theta_0,\\theta_1}}{\\text{minimize}}\\ J(\\theta_0, \\theta_1) $$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Classification \n",
    "Try to separate the data in different classes.\n",
    "\n",
    "Discrete valued output (0, 1). Sometimes you can have more than a binary output. The zero is called the \"Negative class\" which denotes the absence of something and 1 which is called the \"Positive Class\" the presence of something.\n",
    "\n",
    "#### Binary classification problem\n",
    "An initial approach is to take the output of the linear regression and use a threshold for it. If it's greater than 0.5 then predict 1 otherwise 0. However this approach leads to bad results, especially when we have outliers.\n",
    "\n",
    "Additionally the linear regression algorithm outputs values that can be greater than 1 or less than zero.\n",
    "\n",
    "#### Logistic regression\n",
    "For linear regression the hypothesis was $ h_\\theta (x) = \\theta^T x $ \n",
    "\n",
    "For logistic regression we would like our classfier to output the following values.\n",
    "$$ 0 \\leq h_\\theta (x) \\leq 1 $$\n",
    "\n",
    "We modify the linear regression hypothesis and write $ h_\\theta (x) = g( \\theta^T x) $ \n",
    "\n",
    "The function $g$ is the sigmoid function which is defined as follows:\n",
    "\n",
    "$$ g(z) = \\frac{1}{1 + e^{-z}} $$\n",
    "\n",
    "When the hypothesis $h_\\theta (x) $ outputs a number, we treat it as the estimated probability that $ y = 1 $ on input $x$. \n",
    "\n",
    "$$ h_\\theta (x) = \\text{P}(y=1 \\mid x;\\theta) $$\n",
    "\n",
    "Probability that $y=1$ given that $x$ parametrized by $\\theta$.\n",
    "\n",
    "##### Decision boundary\n",
    "* We predict $y=1$ whenever $\\theta^T x \\geq 0$\n",
    "* We predict $y=0$ whenever $\\theta^T x \\lt 0$\n",
    "\n",
    "##### Non-linear decision boundaries\n",
    "\n",
    "#### Cost function for logistic regression\n",
    "\n",
    "For logistic regression we have $m$ examples and the hypothesis is represented by $h_\\theta (x) = \\frac{1}{1 + \\text{e}^{\\theta^T x}}$.\n",
    "\n",
    "We want to use a cost function, but if we define it in a similar way to the one we used in linear regression, the resulting cost function is non-convex as it has multiple minima. \n",
    "\n",
    "$$ J(\\theta) = \\frac{1}{m} \\sum_{i = 1}^m {\\text{cost} (h_\\theta (x^{(i)}), y^{(i)}) } $$\n",
    "\n",
    "We define the cost function as\n",
    "\n",
    "$$ \\text{cost}( h_\\theta(x), y) = \n",
    "    \\begin{cases}\n",
    "    - \\text{log}(h_\\theta(x))\\ &\\text{if}\\ y = 1 \\\\\n",
    "    - \\text{log}(1 - h_\\theta(x))\\ &\\text{if}\\ y = 0 \n",
    "    \\end{cases}\n",
    "$$\n",
    "\n",
    "Because $y = 0$ or $y=1$ we can simplify the cost function and remove the two separate cases.\n",
    "\n",
    "$$ \\text{cost}( h_\\theta(x), y) = -y \\text{log}(h_\\theta(x)) - (1 - y) \\text{log}(1 - h_\\theta(x))    \n",
    "$$\n",
    "\n",
    "$$ \\frac{\\partial }{\\partial \\theta_j}\\ J(\\theta) = \\frac{1}{m} \\sum_{i = 1}^m {(h_{\\theta}(x^{(i)}) - y^{(i)}) x_j^{(i)} }   $$\n",
    "\n",
    "#### Multiclass classification (One vs all classification)\n",
    "In logistic regression we have two classes 0, and 1, but for various problems we may have multiple classes.\n",
    "\n",
    "Take the multiclass problem and split it into several binary classification problems. For each binary problem estimate the parameters $\\theta$ and determine the hypothesis $h^{(i)}_\\theta (x)$ where $i$ is the index of the particular class that we are interested in. For example if we have 3 classes, then $i$ would range between 1 and 3. \n",
    "\n",
    "Once we have produced multiple hypotheses, on a new input we make a prediction by picking the class $i$ that maximizes:\n",
    "\n",
    "\n",
    "$$ \\underset{{i}}{\\text{max}}\\ h_\\theta^{(i)} (x) $$\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "### Overfitting\n",
    "It is the situation where we have too many features and the learned hypothesis may fit the training set very well but fail to generalize to new examples.\n",
    "\n",
    "#### Addressing overfitting\n",
    "* Plotting the hypothesis. But sometimes having too many features might make it difficult to visualize.\n",
    "* **Reduce the number of features**. This can be done manually or we can have a model selection algorithm. However throwing away some of the features means that we are losing some information about the problem.\n",
    "* **Regularization**. In this case we keep all the features but we reduce the magnitude/values of parameters $\\theta_j$. This works well when we have a lot of features, each of which contributes a bit to predicting the value of $y$.\n",
    "\n",
    "#### Regularization\n",
    "The intuition behind regularization is that when we have higher order polynomials we penalize the higher-order $\\theta$ parameters when we try to minimize the square difference of the error by adding extra terms in the minimization problem.\n",
    "\n",
    "The idea is that small values for parameters $\\theta_0, \\dots, \\theta_n$ leads to \"simper\" hypothesis which is less prone to overfit.\n",
    "\n",
    "\n",
    "$$ J(\\theta) = \\frac{1}{2m} \\bigg[  \\sum_{i = 1}^m {(h_{\\theta}(x^{(i)}) - y^{(i)})^2 + \\lambda \\sum_{j = 1}^n  \\theta_j^2 \\bigg] } $$\n",
    "\n",
    "$$ \\underset{{\\theta}}{\\text{minimize}}\\ J(\\theta) $$\n",
    "\n",
    "We implement regularization by adding an extra regularization term in our cost function which penalizes every theta equally. The $\\lambda$ is called the regularization parameter.\n",
    "\n",
    "If the regularization parameter is extremely large, then all the $\\theta$s become close to 0, and the only term that survives is $\\theta_0$ which is akin to drawing a straight line and thus underfitting the data.\n",
    "\n",
    "\n",
    "### Advanced optimization\n",
    "In the discussion around gradient descent we established that given the $ J(\\theta) $ and $ \\frac{\\partial }{\\partial \\theta_j}\\ J(\\theta) $, the algorithm found the optimum.\n",
    "\n",
    "However there are several other optimization algorithms:\n",
    "* Gradient descent\n",
    "* Conjugate gradient\n",
    "* BFGS\n",
    "* L-BFGS\n",
    "\n",
    "Some of the advantages include that:\n",
    "* There's no need to manually pick the learning parameter $\\alpha$\n",
    "* They are often faster than gradient descent\n",
    "* However they are more complex\n",
    "\n",
    "### Neural Networks (Non-linear hypothesis)\n",
    "When you start working on large problems with multiple features, as you start including quadratic or higher-order terms, you end up having a large number of terms in your hypothesis. For example, imagine a problem with 100 features. If you wanted to include second order terms, your hypothesis would have to include terms like $x_1^2, x_1 x_2, x_1 x_3, \\dots$. In general the number of terms grows in the order of $\\text{O}(n^2)$.\n",
    "\n",
    "\n",
    "Each neuron is a \"computational unit\" that takes several inputs and outputs a value computed by the sigmoid function.\n",
    "\n",
    "A neural network is a combination of neurons that are structured in layers, where the inputs of each layer come from the preceding layer and the outputs feed the next layer.\n",
    "\n",
    "The first layer is called the input layer and the last layer is called the output layer.\n",
    "\n",
    "The middle layer is called the \"hidden layer\".\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Unsupervised learning\n",
    "Given a dataset we are asked to find some structure in the data\n",
    "\n",
    "### Clustering\n",
    "\n",
    "### Decompose the voices from a audio clip\n",
    "Cocktail party problem.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "\n",
    "pd.set_option('display.notebook_repr_html', False)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 150)\n",
    "pd.set_option('display.max_seq_items', None)\n",
    " \n",
    "#%config InlineBackend.figure_formats = {'pdf',}\n",
    "%matplotlib inline  \n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_context('notebook')\n",
    "sns.set_style('white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  1.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  1.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.identity(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0xc8c79b0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe0AAAFXCAYAAACP5RboAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xc8d6e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "filename = \"data/ex1data1.txt\"\n",
    "data = np.loadtxt(filename, delimiter=\",\")\n",
    "type(data)\n",
    "#help(pd.read_csv)\n",
    "d = pd.read_csv(filename, header=None) # Header=None indicates no header\n",
    "d.columns = [\"population\", \"profits\"]\n",
    "d\n",
    "\n",
    "plt.xlim(4,24)\n",
    "plt.xlabel(\"Population of City in 10,000s\")\n",
    "plt.ylabel(\"Profit in $10,000s\")\n",
    "plt.scatter(d[\"population\"], d[\"profits\"], s=30, c=\"r\", marker=\"x\", linewidths=1)\n",
    "\n",
    "#def costFunction(x, y, theta=)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient descent algorithm\n",
    "A generalised algorithm for solving minimization problems.\n",
    "\n",
    "At every iteration we perform the following assignment until we converge.\n",
    "\n",
    "$$ \\theta_i := \\theta_i - \\alpha \\frac{\\partial }{\\partial t}\\ J(\\theta_0, \\dots, \\theta_n) $$\n",
    "\n",
    "In words, we compute the partial derivative of the cost function with respect to the variable we are trying to update, compute the cost function and then update the parameter $\\theta$. The parameter $\\alpha$ that appears in the equation is called the learning rate and indicates \"how big\" are the steps we take at every iteration. Note that as we approach a local miminum, the partial derivative term becomes smaller and small because the slope of the curve is smaller, and thus each successive step is smaller even with a fixed $\\alpha$ term.\n",
    "\n",
    "If $\\alpha$ is too small the gradient descent can be slow. If $\\alpha$ is too large, gradient descent can overshoot the minimum, may fail to converge, or even diverge.\n",
    "\n",
    "**Batch gradient descent**\n",
    "This term refers to the fact that at each step of the gradient descent algorithm, we use the entire set of data available to us to compute the derivative of the cost function.\n",
    "\n",
    "#### Extensions\n",
    "We can solve exactly for the parameters above, which would elimitate the need for the parameter $\\alpha$\n",
    "\n",
    "\n",
    "### Regression with multiple features\n",
    "\n",
    "We extend the notation. We use a training set. The number of training examples is denoted by $m$. The input variables are denoted using $x$ and the output variables are denoted with $y$. We denot the number of features by $n$.\n",
    "\n",
    "$(x,y)$ denotes one training example\n",
    "\n",
    "$(x^{(i)}, y^{(i)})$ denotes all features of the i-th training example (vector)\n",
    "\n",
    "$(x_j^{(i)}, y_j^{(i)})$ denotes value of feature $j$ of the i-th training example (single value)\n",
    "\n",
    "The entire training set consists of $m$ training examples and $n$ features.\n",
    "\n",
    "To generalize, the hypothesis now becomes:\n",
    "\n",
    "$$ h_{\\theta}(x) = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\dots + \\theta_n x_n$$\n",
    "\n",
    "Or in matrix notation:\n",
    "\n",
    "$$ h_{\\theta}(x) = \\theta^T x$$\n",
    "\n",
    "$$ \\begin{align}\n",
    "    x &= \\begin{bmatrix}\n",
    "           x_{0} \\\\\n",
    "           x_{1} \\\\\n",
    "           \\vdots \\\\\n",
    "           x_{n}\n",
    "         \\end{bmatrix}\n",
    "  \\end{align} \\in \\mathbb{R}^{n+1} $$\n",
    "$$ \\begin{align}\n",
    "    \\theta &= \\begin{bmatrix}\n",
    "           \\theta_{0} \\\\\n",
    "           \\theta_{1} \\\\\n",
    "           \\vdots \\\\\n",
    "           \\theta_{n}\n",
    "         \\end{bmatrix}\n",
    "  \\end{align} \\in \\mathbb{R}^{n+1} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature scaling\n",
    "This is a techinique to make gradient descent converges faster. The idea is to **make sure features are on a similar scale**.\n",
    "\n",
    "For example, square footage of a house (0-5000), and number of bedrooms (1-5). \n",
    "\n",
    "More generally what we want is to get every feature into approximately $ -1 \\leq x_i \\leq 1$.\n",
    "\n",
    "Practically we can turn these feature values into z-scores or subtract the mean and divide by the range.\n",
    "\n",
    "$$ z = \\frac{x - \\mu}{\\sigma^2} $$\n",
    "\n",
    "$$ x' = \\frac{x - \\mu}{\\text{max}(x) - \\text{min}(x)} $$\n",
    "\n",
    "\n",
    "#### Choosing the learning rate\n",
    "\n",
    "* If $\\alpha$ is too small we may get slow convergence.\n",
    "* If $\\alpha$ is too large, the cost function may not decrese on every iternation, and thus may not converge.\n",
    "\n",
    "Try $\\dots 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1 \\dots$\n",
    "\n",
    "\n",
    "#### Polynomial regression\n",
    "The same variable is raised to higher powers. To solve such problems, we performa a substitution where we create a new variable e.g. $x_2 = x_1^2$ and then solve the normal multi-variate linear regression.\n",
    "\n",
    "### Normal equation for linear regression\n",
    "Instead of solving the linear regression problem iteratively using gradient descent, we can use the normal equation.\n",
    "\n",
    "$$ \\begin{align}\n",
    "    x &= \\begin{bmatrix}\n",
    "           x_{0} \\\\\n",
    "           x_{1} \\\\\n",
    "           \\vdots \\\\\n",
    "           x_{n}\n",
    "         \\end{bmatrix}\n",
    "  \\end{align} \\in \\mathbb{R}^{n+1} $$\n",
    "$$ \\begin{align}\n",
    "    \\theta &= \\begin{bmatrix}\n",
    "           \\theta_{0} \\\\\n",
    "           \\theta_{1} \\\\\n",
    "           \\vdots \\\\\n",
    "           \\theta_{n}\n",
    "         \\end{bmatrix}\n",
    "  \\end{align} \\in \\mathbb{R}^{n+1} $$\n",
    "\n",
    "$$ \\theta = (X^T X)^{-1} X^T y $$\n",
    "\n",
    "#### Advantages and disadvantages of gradient descent vs. normal equation\n",
    "\n",
    "**Gradient Descent**\n",
    "* Need to choose learning rate $\\alpha$\n",
    "* Needs many iterations\n",
    "* Works well even with a large number of features (large $n$)\n",
    "\n",
    "** Normal equation**\n",
    "* No need to choose a learning rate $\\alpha$\n",
    "* Don't need to iterate\n",
    "* Need to compute the inverse $(X^T X)^{-1}$ which can be slow for very large $n$. The cost of computing the inverse is in $\\text{O}(n^3)$\n",
    "\n",
    "\n",
    "#### When is $X^T X$ non-invertible (or degenerate)\n",
    "\n",
    "* Redundant features\n",
    "This occurs when you have redundant features, i.e. features that are linearly dependent (e.g. length in feet and length in meters)\n",
    "\n",
    "* Too many features ($m \\leq n$). More features $n$ than samples $m$. Delete some features or use regularization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
